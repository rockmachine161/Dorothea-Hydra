{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno se va a presentar el apartado en el que el proyecto comienza a utilizar las librerías python de pandas y también el uso de Scikit-Learn:\n",
    "* 1.- Cargar los archivos .csv de las pruebas recogidas, tanto benignas como malignas.\n",
    "* 2.- Realizar la unión de ambos archivos en uno solo, añadiendo una columna final con valores \"0\" para tráfico benigno y \"1\" para tráfico maligno\n",
    "* 3.- Eliminar columnas innecesarias que producen ruido\n",
    "* 4.- Completar el archivo final para el estudio realizando la normalización usando MinMaxScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización del documento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar archivos\n",
    "\n",
    "En este primer paso, se procede a cargar los archivos generados tras la ejecución de Dorothea tanto con la opción \"-t attack\" como con \"-t normal\"\n",
    "Se generan una serie de interacciones en el sistemas que permiten generar archivos .csv con todo el flujo de datos de red necesario para el estudio.\n",
    "Tras generar dichos .csv, estos se cargan desde el script gestionarchivos.py de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo CSV\n",
    "maligno = \"/home/osboxes/Documents/Dorothea/Labs/lab_attacks/results/Pruebas/Maligno4.csv\"\n",
    "benigno = \"/home/osboxes/Documents/Dorothea/Labs/lab_normal/results/benigno1.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificación y unión de archivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se agregan los valores \"0\" para tráfico benigno y \"1\" para tráfico maligno en los archivos .csv anteriormente indicados. Para ello, en primer lugar se crean dos variables con dichos valores para posteriormente agregarlos a los archivos .csv cargados en una nueva columna denominada \"Tipo Trafico\". Se finaliza este paso unificando ambos archivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor fijo a añadir en la nueva columna\n",
    "valor_maligno = 1\n",
    "valor_benigno = 0\n",
    "\n",
    "# Cargar archivos CSV\n",
    "df1 = pd.read_csv(maligno)\n",
    "df2 = pd.read_csv(benigno)\n",
    "\n",
    "# Agregar una columna con un valor fijo\n",
    "df1['Tipo Trafico'] = valor_maligno\n",
    "df2['Tipo Trafico'] = valor_benigno\n",
    "\n",
    "# Se unen los archivos en uno\n",
    "df_concatenado = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de columnas innecesarias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una mejor gestión de los datos y una mejor comprensión, se proceden a eliminar las columnas que introducen ruido en las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan columnas innecesarias\n",
    "columnas_eliminar = ['engine_type', 'engine_id', 'tos', 'src_mask', 'dst_mask', 'src_as', 'dst_as' ]  \n",
    "df_concatenado = df_concatenado.drop(columnas_eliminar, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestión de las columnas que contienen direcciones IP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para normalizar las columnas del archivo .csv que tienen formato de dirección IP, en primer lugar se transforman dichas columnas a un formato de Bytes para posteriormente pasarlas a enteros.\n",
    "Para ello se define la función normalize_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la función que realizará la conversión de las IPs a enteros\n",
    "def normalize_ip(ip):\n",
    "    try:\n",
    "        # Convierte la dirección IP en formato de bytes\n",
    "        packed_ip = socket.inet_aton(ip)\n",
    "        packed_ip = int.from_bytes(packed_ip, byteorder='big')\n",
    "        return packed_ip\n",
    "    except socket.error:\n",
    "        # Si la dirección IP es inválida, devuelve None\n",
    "        return None\n",
    "\n",
    "# Se definen las columnas del archivo que tienen las direcciones IP que se tienen que normalizar\n",
    "DireccionesIP = [\"exaddr\", \"srcaddr\", \"dstaddr\", \"nexthop\"]\n",
    "for direccion in DireccionesIP:\n",
    "    df_concatenado[direccion] = df_concatenado[direccion].apply(normalize_ip)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivo normalizado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar este primer apartado, se procede con la normalización del documento a través de la función MinMaxScaler() facilitada por la libreria Scikit-Learn.\n",
    "De esta manera, se realizará el entrenamiento de nuestra inteligencia con valores entre 0 y 1, permitiendo así, que las pruebas realizadas no sufran errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza ahora la normalización se las columnas para \n",
    "# posteriormente trabajar con ellas\n",
    "\n",
    "columnas = ['#:unix_secs', 'unix_nsecs', \"exaddr\", 'sysuptime', 'dpkts', 'doctets', 'first', 'last', \"srcaddr\", \"dstaddr\", \"nexthop\", \"input\", 'output', 'srcport', 'dstport', 'prot', 'tcp_flags', 'Tipo Trafico']\n",
    "scaler = MinMaxScaler()\n",
    "df_concatenado[columnas] = scaler.fit_transform(df_concatenado[columnas])\n",
    "df_concatenado.to_csv('csv_normalizado.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
